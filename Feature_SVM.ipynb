{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37675</th>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44816</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49851</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89320</th>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93890</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment  year  logged_in  \\\n",
       "rev_id                                                                       \n",
       "37675   `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "44816   `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002      False   \n",
       "49851   NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002      False   \n",
       "89320    Next, maybe you could work on being less cond...  2002       True   \n",
       "93890                This page will need disambiguation.   2002       True   \n",
       "\n",
       "             ns  sample  split  \n",
       "rev_id                          \n",
       "37675   article  random  train  \n",
       "44816   article  random  train  \n",
       "49851   article  random  train  \n",
       "89320   article  random    dev  \n",
       "93890   article  random  train  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37675</td>\n",
       "      <td>1362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37675</td>\n",
       "      <td>2408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37675</td>\n",
       "      <td>1493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37675</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37675</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  worker_id  quoting_attack  recipient_attack  third_party_attack  \\\n",
       "0   37675       1362             0.0               0.0                 0.0   \n",
       "1   37675       2408             0.0               0.0                 0.0   \n",
       "2   37675       1493             0.0               0.0                 0.0   \n",
       "3   37675       1439             0.0               0.0                 0.0   \n",
       "4   37675        170             0.0               0.0                 0.0   \n",
       "\n",
       "   other_attack  attack  \n",
       "0           0.0     0.0  \n",
       "1           0.0     0.0  \n",
       "2           0.0     0.0  \n",
       "3           0.0     0.0  \n",
       "4           0.0     0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.6\n",
    "# labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37675</th>\n",
       "      <td>`- This is not ``creative``.  Those are the di...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44816</th>\n",
       "      <td>`  :: the term ``standard model`` is itself le...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49851</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89320</th>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93890</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment  year  logged_in  \\\n",
       "rev_id                                                                       \n",
       "37675   `- This is not ``creative``.  Those are the di...  2002      False   \n",
       "44816   `  :: the term ``standard model`` is itself le...  2002      False   \n",
       "49851     True or false, the situation as of March 200...  2002      False   \n",
       "89320    Next, maybe you could work on being less cond...  2002       True   \n",
       "93890                This page will need disambiguation.   2002       True   \n",
       "\n",
       "             ns  sample  split  attack  \n",
       "rev_id                                  \n",
       "37675   article  random  train   False  \n",
       "44816   article  random  train   False  \n",
       "49851   article  random  train   False  \n",
       "89320   article  random    dev   False  \n",
       "93890   article  random  train   False  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    0.600066\n",
       "test     0.200045\n",
       "dev      0.199890\n",
       "Name: split, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of train, test, and dev data in original data\n",
    "comments.split.value_counts() / len(comments.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split training, development and testing\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "dev_comments = comments.query(\"split=='dev'\")\n",
    "test_comments = comments.query(\"split=='test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of rev_id, year, logged_in, ns, sample column that will not be used in training\n",
    "# only keep comments as training feature\n",
    "def get_X_Y(data):\n",
    "    X = data.comment\n",
    "    Y = data.iloc[:, -1]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = get_X_Y(train_comments)\n",
    "X_dev, Y_dev = get_X_Y(dev_comments)\n",
    "X_test, Y_test = get_X_Y(test_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make sur nltk is downloaded\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.data.path = ['/Users/liangpengzhuang/Downloads/nltk_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code reference https://www.kaggle.com/c/word2vec-nlp-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean comment string, and return a list of words\n",
    "# Only consider character and number\n",
    "def comment_to_wordlist(review, remove_stopwords=True):\n",
    "    # Remove non-letters \n",
    "    review = re.sub(\"[^a-zA-Z ]\",\" \", review)\n",
    "    words = review.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "# tokenizer = nltk.data.load('/Users/liangpengzhuang/Downloads/nltk_data/tokenizers/punkt/english.pickle')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "# function to split a review into parsed sentences. Comments will be divided to sentences first, and then transform to \n",
    "# list of words\n",
    "def review_to_sentences( comment, tokenizer, remove_stopwords):\n",
    "    # Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(comment.strip())\n",
    "\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( comment_to_wordlist(raw_sentence, remove_stopwords))\n",
    "            \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram features\n",
    "### cleanning method 1 - Liangpeng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(comment):\n",
    "    words = comment_to_wordlist(comment)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_clean = X_train.apply(clean_text)\n",
    "X_test_clean = X_test.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanning method 2 - Kingston\n",
    " true recall + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "def text_clean(text):\n",
    "    tokens = word_tokenize(text)\n",
    "#     tokens = [w.replace(\"NEWLINE_TOKEN\", \" \") for w in tokens]\n",
    "#     tokens = [w.replace(\"TAB_TOKEN\", \" \") for w in tokens]\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    stop_words_list = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words_list]\n",
    "    filted_text = ' '.join(tokens)\n",
    "    return filted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_clean = X_train.apply(text_clean)\n",
    "X_test_clean = X_test.apply(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_char = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "clf_char = clf_char.fit(X_train_clean, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.94      0.99      0.97     20913\n",
      "       True       0.87      0.42      0.56      2265\n",
      "\n",
      "avg / total       0.93      0.94      0.93     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = clf_char.predict(X_test_clean)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "conf = confusion_matrix (Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level\n",
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.95      0.96     20913\n",
      "       True       0.63      0.81      0.71      2265\n",
      "\n",
      "avg / total       0.95      0.94      0.94     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_word = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', max_features = 10000, ngram_range = (1,1))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LinearSVC(\n",
    "#         dual=False,\n",
    "#         tol = 0.01,\n",
    "#         loss='hinge',\n",
    "        class_weight='balanced'\n",
    "    )),\n",
    "])\n",
    "clf_word = clf_word.fit(X_train_clean, Y_train)\n",
    "Y_pred = clf_word.predict(X_test_clean)\n",
    "conf = confusion_matrix (Y_test, Y_pred)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_word = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', svm.SVC()),\n",
    "])\n",
    "clf_word = clf_word.fit(X_train_clean, Y_train)\n",
    "Y_pred = clf_word.predict(X_test_clean)\n",
    "conf = confusion_matrix (Y_test, Y_pred)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "print('Confusion matrix: ', conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tunning\n",
    "### LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 10000, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LinearSVC(verbose=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': [1.0],\n",
      " 'clf__class_weight': [None],\n",
      " 'clf__loss': ['hinge', 'squared_hinge'],\n",
      " 'clf__tol': [0.01, 0.0001]}\n"
     ]
    }
   ],
   "source": [
    "param_grid = { \n",
    "#             'clf__penalty':['l1','l2'],\n",
    "            'clf__loss':['hinge','squared_hinge'],\n",
    "#             'clf__dual':[False],\n",
    "            'clf__tol':[1e-2,1e-4],\n",
    "           \"clf__C\":[1.0],\n",
    "#            \"clf__kernel\" : ['rbf', 'linear', 'poly','sigmoid','precomputed'],\n",
    "#            \"clf__degree\" : [1, 2, 3, 4, 5],\n",
    "           'clf__class_weight':[None],\n",
    "}\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liangpengzhuang/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/liangpengzhuang/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        stri...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'clf__loss': ['hinge', 'squared_hinge'], 'clf__tol': [0.01, 0.0001], 'clf__C': [1.0], 'clf__class_weight': [None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs_clf = GridSearchCV(estimator=text_clf, param_grid=param_grid, n_jobs=-1)\n",
    "gs_clf.fit(X_train_clean, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9533699623162558\n",
      "clf__C: 1.0\n",
      "clf__class_weight: None\n",
      "clf__loss: 'hinge'\n",
      "clf__tol: 0.01\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf.best_score_)\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 10000, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', svm.SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "           \"clf__C\" : [0,1,2,3],\n",
    "           \"clf__kernel\" : ['rbf', 'linear', 'poly','sigmoid','precomputed'],\n",
    "           \"clf__degree\" : [1, 2, 3, 4, 5],\n",
    "           'clf__tol':[1e-2,1e-1,1e-3],\n",
    "            'class_weight':['balanced',None]\n",
    "}\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmer\n",
    "Stemmers remove morphological affixes from words, leaving only the word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_stem(comment):\n",
    "    words = comment_to_wordlist(comment)\n",
    "    words_out = []\n",
    "    for word in words:\n",
    "        words_out.append(ps.stem(word))\n",
    "    return ' '.join(words_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_stem = X_train.apply(word_to_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_stem = X_test.apply(word_to_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.96      0.99      0.97     20913\n",
      "       True       0.86      0.63      0.72      2265\n",
      "\n",
      "avg / total       0.95      0.95      0.95     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_word_stem = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "clf_word_stem = clf_word_stem.fit(X_train_stem , Y_train)\n",
    "Y_pred = clf_word_stem.predict(X_test_stem)\n",
    "conf = confusion_matrix (Y_test, Y_pred)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer\n",
    "Lemmatize using WordNet's built-in morphy function.\n",
    "Returns the input word unchanged if it cannot be found in WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_lemm(comment, tokenizer, remove_stopwords=True):\n",
    "    sentences = review_to_sentences(comment, tokenizer, remove_stopwords)\n",
    "    feature_str = ''\n",
    "    for sent in sentences:\n",
    "        tag = pos_tag(sent)\n",
    "        for tuple_pair in tag:\n",
    "            pos = get_wordnet_tag(tuple_pair[1])\n",
    "            if len(pos) > 0:\n",
    "                feature_str += lemmatizer.lemmatize(tuple_pair[0], pos=pos) + ' '\n",
    "            else:\n",
    "                feature_str += lemmatizer.lemmatize(tuple_pair[0]) + ' '\n",
    "    return feature_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lemm = X_train.apply(lambda x: word_lemm(x, tokenizer))\n",
    "X_test_lemm = X_test.apply(lambda x: word_lemm(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.96      0.99      0.97     20913\n",
      "       True       0.86      0.62      0.72      2265\n",
      "\n",
      "avg / total       0.95      0.95      0.95     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_word_lemm = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "clf_word_lemm = clf_word_lemm.fit(X_train_lemm , Y_train)\n",
    "Y_pred = clf_word_lemm.predict(X_test_lemm)\n",
    "conf = confusion_matrix (Y_test, Y_pred)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"22877a2b-c18a-48b6-bc34-42a8abde8222\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"22877a2b-c18a-48b6-bc34-42a8abde8222\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"cool\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.96      0.99      0.97     20913\n",
      "       True       0.86      0.62      0.72      2265\n",
      "\n",
      "avg / total       0.95      0.95      0.95     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% notify -m 'cool'\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding derived features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code reference: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_total, Y_total = get_X_Y(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a list of all sentences in over comments for word2vec model training\n",
    "def get_all_sent(data):\n",
    "    all_sent = []\n",
    "    for idx, value in data.iteritems():\n",
    "        all_sent += review_to_sentences(value, tokenizer, remove_stopwords=False)\n",
    "    return all_sent\n",
    "all_sent = get_all_sent(X_total)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a w2v model using all comments \n",
    "def train_w2v(sentences, num_features):\n",
    "    model = word2vec.Word2Vec(sentences, sg=1, workers=4, \\\n",
    "            size=num_features, min_count = 1, \\\n",
    "            window = 10, sample = 1e-3, iter=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model with 300 features\n",
    "w2v_model = train_w2v(all_sent, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all word vectors from trained model\n",
    "word_vectors = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model.save('w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_w2v = X_train.apply(lambda x: comment_to_wordlist(x,remove_stopwords=False))\n",
    "X_test_w2v = X_test.apply(lambda x: comment_to_wordlist(x,remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2vec return a 300 dimension vector for each word in the comment, to transfrom a entire comment to a vector\n",
    "# Averaging word vectors for all words in a text.\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = 300\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.99      0.97     20913\n",
      "       True       0.88      0.52      0.65      2265\n",
      "\n",
      "avg / total       0.94      0.95      0.94     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_w2v_self = Pipeline([\n",
    "    ('w2v-embed', MeanEmbeddingVectorizer(word_vectors)),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "clf_w2v_self = clf_w2v_self.fit(X_train_w2v, Y_train)\n",
    "Y_pred = clf_w2v_self.predict(X_test_w2v)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "conf = confusion_matrix(Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar with previous MeanEmbeddingVectorizer, instead, using Tfidf\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim=300\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.99      0.97     20913\n",
      "       True       0.88      0.51      0.65      2265\n",
      "\n",
      "avg / total       0.94      0.95      0.94     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_w2v_self_tfid = Pipeline([\n",
    "    ('w2v-embed', TfidfEmbeddingVectorizer(word_vectors)),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "clf_w2v_self_tfid = clf_w2v_self_tfid.fit(X_train_w2v, Y_train)\n",
    "Y_pred = clf_w2v_self_tfid.predict(X_test_w2v)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "conf = confusion_matrix(Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n",
    "#### Self-trained Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_total, Y_total = get_X_Y(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_total_d2v = X_total.apply(lambda x: comment_to_wordlist(x, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tagged doc is the required input format for doc2vec training\n",
    "# Each tagged document has a list of words, and a unique tag for furture vector retrival\n",
    "def build_taggedDoc(data):\n",
    "    data_total = []\n",
    "    for idx, words in data.iteritems():\n",
    "        doc = TaggedDocument(words, ['SENT' + str(idx)])\n",
    "        data_total.append(doc)\n",
    "    return data_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged = build_taggedDoc(X_total_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_d2v(lr, T, data):\n",
    "    model = Doc2Vec(vector_size=300, min_count=1, sample=len(data), alpha=lr, min_alpha=lr, epochs=T)\n",
    "    model.build_vocab(data)\n",
    "    model.train(data, epochs=model.epochs, total_examples=model.corpus_count)\n",
    "    return model\n",
    "d2v_model = train_d2v(0.025, 20, tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vector(d2v_model, doc_data):\n",
    "    x_data = []\n",
    "    for idx, words in doc_data.iteritems():\n",
    "        label = 'SENT' + str(idx)\n",
    "        x_data.append(d2v_model.docvecs[label])\n",
    "    return np.array(x_data)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_d2v, Y_train = get_X_Y(train_comments)\n",
    "X_test_d2v, Y_test = get_X_Y(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_d2v = X_train_d2v.apply(lambda x: comment_to_wordlist(x, remove_stopwords=True))\n",
    "X_test_d2v = X_test_d2v.apply(lambda x: comment_to_wordlist(x, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_d2v_v = get_vector(d2v_model, X_train_d2v)\n",
    "X_test_d2v_v = get_vector(d2v_model, X_test_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.92      0.99      0.96     20913\n",
      "       True       0.74      0.24      0.36      2265\n",
      "\n",
      "avg / total       0.90      0.92      0.90     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_d2v = LinearSVC()\n",
    "clf_d2v.fit(X_train_d2v_v, Y_train)\n",
    "Y_pred =clf_d2v.predict(X_test_d2v_v)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "conf = confusion_matrix(Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Syntactic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text and part-of-speech(POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instead word as feature, and its POS tag as addition feature\n",
    "# Eg: 'I like cat' -> 'i i_NNS like like_VBP cat cat_NN'\n",
    "def get_pos_feature(data, tokenizer, remove_stopwords=False):\n",
    "    sentences = review_to_sentences(data, tokenizer, remove_stopwords)\n",
    "    feature_str = ''\n",
    "    for sent in sentences:\n",
    "        tag = pos_tag(sent)\n",
    "        for tuple_pair in tag:\n",
    "            feature_str += tuple_pair[0] + ' '\n",
    "            feature_str += tuple_pair[0] + '_' + tuple_pair[1] + ' '\n",
    "    return feature_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_pos = X_train.apply(lambda x: get_pos_feature(x, tokenizer))\n",
    "X_test_pos = X_test.apply(lambda x: get_pos_feature(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.96      0.99      0.98     20913\n",
      "       True       0.88      0.63      0.73      2265\n",
      "\n",
      "avg / total       0.95      0.96      0.95     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_pos_tag = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "clf_pos_tag =clf_pos_tag.fit(X_train_pos, Y_train)\n",
    "Y_pred = clf_pos_tag.predict(X_test_pos)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "conf = confusion_matrix (Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets = set(['NN','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ','PRP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only consider Noun, Verb, Adjective, Pronoun \n",
    "def get_pos_feature_wo(data, tokenizer, remove_stopwords=False):\n",
    "    sentences = review_to_sentences(data, tokenizer, remove_stopwords)\n",
    "    feature_str = ''\n",
    "    for sent in sentences:\n",
    "        tag = pos_tag(sent)\n",
    "        for tuple_pair in tag:\n",
    "            if tuple_pair[1] in sets:\n",
    "                # Ignore the tense in tagging\n",
    "                # Eg: both took, take will be tagged 'V'\n",
    "                if 'V' in tuple_pair[1]:\n",
    "                    feature_str += tuple_pair[0] + '_' + 'V' + ' '\n",
    "                # Ignore comparable in adjective\n",
    "                # Eg: Both clean, cleaner, cleanest will be tagged 'J'\n",
    "                elif 'J' in tuple_pair[1]:\n",
    "                    feature_str += tuple_pair[0] + '_' + 'J' + ' '\n",
    "                else:\n",
    "                    feature_str += tuple_pair[0] + '_' + tuple_pair[1] + ' '\n",
    "    return feature_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_pos_wo = X_train.apply(lambda x: get_pos_feature_wo(x, tokenizer))\n",
    "X_test_pos_wo = X_test.apply(lambda x: get_pos_feature_wo(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.96      0.99      0.97     20913\n",
      "       True       0.86      0.61      0.71      2265\n",
      "\n",
      "avg / total       0.95      0.95      0.95     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_pos_tag_wo = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "clf_pos_tag_wo =clf_pos_tag_wo.fit(X_train_pos_wo , Y_train)\n",
    "Y_pred = clf_pos_tag_wo.predict(X_test_pos_wo)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "conf = confusion_matrix (Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine BOW with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_vectorize = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_b = clf_vectorize.fit_transform(X_train_clean).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_b = clf_vectorize.fit_transform(X_test_clean).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_em = MeanEmbeddingVectorizer(word_vectors).transform(X_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_em = MeanEmbeddingVectorizer(word_vectors).transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_br = np.apply_along_axis(lambda x: np.squeeze(np.asarray(x)), 1, x_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_br = np.apply_along_axis(lambda x: np.squeeze(np.asarray(x)), 1, x_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_be = np.c_[x_train_br, x_train_em]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_be = np.c_[x_test_br, x_test_em]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.91      0.98      0.95     20913\n",
      "       True       0.49      0.15      0.23      2265\n",
      "\n",
      "avg / total       0.87      0.90      0.88     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svc = LinearSVC()\n",
    "clf_svc.fit(x_train_be, Y_train)\n",
    "Y_pred = clf_svc.predict(x_test_be)\n",
    "auc = accuracy_score(Y_test, Y_pred)\n",
    "conf = confusion_matrix (Y_test, Y_pred)\n",
    "print(metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
